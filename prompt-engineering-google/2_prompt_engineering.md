# Prompt Engineering
An LLM works as a prediction engine.  The model takes sequential text as an input and predicts what the following token should be, based on the data it was trained on.

When you write a prompt, you're attempting to set the LLM up to predict the right sequence of tokens.  Prompt engineering is the process of designing high-quality prompts that guide LLMs to produce accurate outputs.  The process involves tinker to find the best prompt, optimizing prompt length, and evaluating a prompt's writing style and structure in relation to the task.  